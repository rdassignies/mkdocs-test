Cette page référence uniquement les grands modèles de langage pré entraînés actuels. Pour une liste de plusieurs dizaines de millier de modèles de language pré entraînés, il faut consulter [le site d'HuggingFace](https://huggingface.co/models)

## LLaMa (Meta AI)
https://research.facebook.com/file/1574548786327032/LLaMA--Open-and-Efficient-Foundation-Language-Models.pdf
**Abstract**
We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla70B and PaLM-540B. We release all our models to the research community1

## Cedille 
https://arxiv.org/abs/2202.03371 
**Abstract**

GPT J, BLOOM,  GPT (openAI)

GPT (openAI) : 
Codex (openAI) : 
OPT(Meta) : 
PALM (Google): à creuser
BLOOM (Huggingface) : 
GPT-NeoX (Eleuther AI) : 
GPT-J
