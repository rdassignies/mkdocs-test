{"config":{"lang":["fr"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Bienvenue sur legalGPT !","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"LLM_Liste/","title":"LLM Liste","text":"<p>Cette page r\u00e9f\u00e9rence uniquement les grands mod\u00e8les de langage pr\u00e9 entra\u00een\u00e9s actuels. Pour une liste de plusieurs dizaines de millier de mod\u00e8les de language pr\u00e9 entra\u00een\u00e9s, il faut consulter le site d'HuggingFace</p>"},{"location":"LLM_Liste/#llama-meta-ai","title":"LLaMa (Meta AI)","text":"<p>https://research.facebook.com/file/1574548786327032/LLaMA--Open-and-Efficient-Foundation-Language-Models.pdf Abstract We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla70B and PaLM-540B. We release all our models to the research community1</p>"},{"location":"LLM_Liste/#cedille","title":"Cedille","text":"<p>https://arxiv.org/abs/2202.03371  Abstract</p> <p>GPT J, BLOOM,  GPT (openAI)</p> <p>GPT (openAI) :  Codex (openAI) :  OPT(Meta) :  PALM (Google): \u00e0 creuser BLOOM (Huggingface) :  GPT-NeoX (Eleuther AI) :  GPT-J</p>"},{"location":"page2/","title":"Page 2","text":""},{"location":"page2/#head-2","title":"Head 2","text":"<p>du texte en markdown</p>"},{"location":"R%C3%A9gulation/Glossaire/","title":"Glossaire","text":"<p>!! Les modifications de Rapha\u00ebl !! ((( les modifications de beatrice))) Voicile vocabulaire [^1] \u00e0 conna\u00eetre +++ pour d\u00e9crypter le domaine. Il est class\u00e9 par th\u00e9matiques. Ce classement, totalement arbitraire,  Entit\u00e9 nomm\u00e9 (NER)</p> <p>Embedding</p>"},{"location":"R%C3%A9gulation/Glossaire/#machine-learning","title":"Machine learning","text":""},{"location":"R%C3%A9gulation/Glossaire/#parametres","title":"Param\u00e8tres :","text":"<p>Les param\u00e8tres d'un mod\u00e8le designent les variables qui sont apprises par le mod\u00e8le lors de la</p> <p>phase d'apprentissage. </p>"},{"location":"R%C3%A9gulation/Glossaire/#hyperparametres","title":"Hyperparam\u00e8tres :","text":"<p>On destingue les hyperparam\u00e8tres du mod\u00e8le (nombre de couches, taille, ... ) et ceux li\u00e9s \u00e0 l'apprentissage (learning rate, optimizer, ... ). </p> <p>Transfert d'apprentissage L\u2019apprentissage par transfert peut \u00eatre vu comme la capacit\u00e9 d\u2019un syst\u00e8me \u00e0 reconna\u00eetre et \u00e0 appliquer des connaissances et des comp\u00e9tences, apprises \u00e0 partir de t\u00e2ches ant\u00e9rieures, sur de nouvelles t\u00e2ches ou domaines partageant des similitudes</p> <p>D\u00e9pendances (dependencies)</p> <p>Part Of Speech</p> <p>Stopwords </p> <p>NLP </p> <p>Prompting</p> <p>GPT (Generative Pre-trained Transformer)</p> <p>Tokenizer </p> <p>Transformer </p> <p>Attention</p> <p>Self Attention </p> <p>Sparse matrice  Bag of words  Dense vectors Fine tuning </p> <p>RAG  in_context learning zero shot learning few shots learning</p> <p>Precision Recall  F1 score Perplexit\u00e9 </p> <p>RNN LSTM GRU Logits</p> <p>Standard Q&amp;A Open Q&amp;A Generative Q&amp;A</p> <p>TF-IDF</p> <p>BM25</p> <p>Jacard (distance)</p> <p>Normalisation</p> <p>Token</p> <p>BPE</p> <p>UTF</p> <p>Ontologie  Knowledge graph</p>"},{"location":"R%C3%A9gulation/Glossaire/#transformers","title":"Transformers","text":""},{"location":"R%C3%A9gulation/Glossaire/#les-notions-cles","title":"Les notions cl\u00e9s","text":""},{"location":"R%C3%A9gulation/Glossaire/#attention","title":"Attention","text":""},{"location":"R%C3%A9gulation/Glossaire/#encoder","title":"Encoder","text":""},{"location":"R%C3%A9gulation/Glossaire/#decoder","title":"Decoder","text":""},{"location":"R%C3%A9gulation/Glossaire/#les-architectures","title":"Les architectures","text":""},{"location":"R%C3%A9gulation/Glossaire/#encoders-or-autoencoding-models","title":"Encoders or autoencoding models","text":"<p>As mentioned before, these models rely on the encoder part of the original transformer and use no mask so the model can look at all the tokens in the attention heads. For pretraining, targets are the original sentences and inputs are their corrupted versions. Exemples : BERT, CamemBERT, FlauBERT, JuriBERT</p>"},{"location":"R%C3%A9gulation/Glossaire/#decoders-autoregressive-model-ar-for-language-modelling","title":"Decoders : Autoregressive Model (AR) for Language Modelling :","text":"<p>mod\u00e8le qui pr\u00e9dit les valeurs futures \u00e0 partir de valeurs pass\u00e9es. GPT fait partie de ce type de mod\u00e8le.  Exemples : GPT J, BLOOM,  GPT (openAI) Cedille : https://arxiv.org/abs/2202.03371  GPT (openAI) :  Codex (openAI) :  OPT(Meta) :  PALM (Google): \u00e0 creuser BLOOM (Huggingface) :  GPT-NeoX (Eleuther AI) :  GPT-J</p>"},{"location":"R%C3%A9gulation/Glossaire/#encoder-decoder-sequence-to-sequence-models","title":"Encoder - Decoder : Sequence-to-sequence models","text":"<p>As mentioned before, these models keep both the encoder and the decoder of the original transformer. Exemple : T5</p>"},{"location":"R%C3%A9gulation/Glossaire/#sentence-transformer","title":"Sentence Transformer","text":"<p>Transformers Encoder Decoder  Attention</p> <p>Librairies logicielles sp\u00e9cialis\u00e9es dans la recherche d'informations :  Elasticsearch Faiss Qdrant Pinecone</p> <p>Services en ligne de NLP :  Voici une liste des services proposant, via des api, l'utilisation de mod\u00e8les d\u00e9j\u00e0 h\u00e9berg\u00e9s. J'ai volontairement \u00e9cart\u00e9 les solutions de cloud des g\u00e9ants du secteur (Azur, Google, Amazon). </p> <p>Liste non exhaustive des LLM : </p> <p>Similarit\u00e9 cosinus </p> <p>Distance euclidienne</p> <p>BERT </p> <p>Encoder </p> <p>Decoder</p> <p>Dataset (jeu de donn\u00e9es) : ensemble de donn\u00e9es, structur\u00e9es ou non, utilis\u00e9e par des algorithmes d'apprentissage machine. </p> <p>Donn\u00e9es structur\u00e9es : donn\u00e9es ayant \u00e9t\u00e9 format\u00e9es en vue d'un traitement algorithmique sp\u00e9cifique. Par exemple, les jeux de donn\u00e9es permettant d'identifier des entit\u00e9s dans un texte comme des r\u00e9f\u00e9rences l\u00e9gales.  Donn\u00e9es non structur\u00e9e : donn\u00e9es non format\u00e9es en vue d'une exploitation directe par un algorithme. Il peut s'agir de documents comme des pdf, emails, images. </p> <p>Legal NLP</p> <p>[^1] : footnote</p>"},{"location":"R%C3%A9gulation/LLM_Liste/","title":"LLM Liste","text":"<p>Cette page r\u00e9f\u00e9rence uniquement les grands mod\u00e8les de langage pr\u00e9 entra\u00een\u00e9s actuels. Pour une liste de plusieurs dizaines de millier de mod\u00e8les de language pr\u00e9 entra\u00een\u00e9s, il faut consulter le site d'HuggingFace</p>"},{"location":"R%C3%A9gulation/LLM_Liste/#llama-meta-ai","title":"LLaMa (Meta AI)","text":"<p>https://research.facebook.com/file/1574548786327032/LLaMA--Open-and-Efficient-Foundation-Language-Models.pdf Abstract We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla70B and PaLM-540B. We release all our models to the research community1</p>"},{"location":"R%C3%A9gulation/LLM_Liste/#cedille","title":"Cedille","text":"<p>https://arxiv.org/abs/2202.03371  Abstract</p> <p>GPT J, BLOOM,  GPT (openAI)</p> <p>GPT (openAI) :  Codex (openAI) :  OPT(Meta) :  PALM (Google): \u00e0 creuser BLOOM (Huggingface) :  GPT-NeoX (Eleuther AI) :  GPT-J</p>"},{"location":"mon%20repertoire/test/","title":"Test","text":"Method Description <code>GET</code>      Fetch resource <code>PUT</code>  Update resource <code>DELETE</code>      Delete resource <p><code>essai</code> </p>"}]}